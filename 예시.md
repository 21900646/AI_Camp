# Multi Layer ed Perceptron(MLP)
MLP 학습
![image](https://user-images.githubusercontent.com/69943167/125712332-e7544075-9549-47d1-a782-382ad4fee442.png)

학습 규칙 : 오류 역전파 학습(Error Back Propagation)
![image](https://user-images.githubusercontent.com/69943167/125712370-a5702240-5113-4958-956d-1eee51ca02b4.png)
![image](https://user-images.githubusercontent.com/69943167/125712387-2ba438cf-b1d3-4055-98fd-c8ca4201ec6b.png)

## 필기체 숫자 인식에 적용
1. 자료 준비
```
from sklearn.datasets import load_digits
X, y = load_digits(return_X_y = True)
print(len(X))
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)
```

2. 모델 import와 인스턴스화

  바이너리를 하고, solver를 사용했다.
```
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(solver='sgd', activation='logistic', hidden_layer_sizes=(30,10), random_state=1, max_iter=2000)
```

3. 학습 데이터로 학습
```
mlp.fit(X_train, y_train)
```

4. 성능평가 (score - 얼마나 train이 잘 됐는지, test가 잘 됐는지)
```
mlp.score(X_train, y_train)
mlp.score(X_test, y_test)
```

## MLP 직접 구현(MLP XOR)
```
def train(self):
  for _ in range(self.num_epochs):
    self.forward(self.train_data)
    self.update_weights()

def sigmoid(self, x):
  return 1/(1+np.exp(-x))

def _delsigmoid(self, x):
  return x*(1-x)

def forward(self, batch):
  self.hidden_ = np.dot(batch, self.weights_01)+self.b01
  self.hidden_out = self._sigmoid(self.hidden_)

  self.output = np.dot(self.hidden_out, self.weights_12) + self.b12
  self.output_final = self._sigmoid(self.output_)

  return self.output_final

def update_weights(self):
  # squared error 계산하기
  loss = 0.5 * (self.target - self.output_final) ** 2
  print(loss)
  self.losses.append(np.sum(loss))

  error_term = (self.target - self.output_final)

  # gradient for the hidden layer weights
  grad01 = self.train_data.T @ (((error_term * self._delsigmoid(self.output_final)) * self.weight_12.T) * self._delsigmoid(self.hidden_out))
  print("grad01: ", grad01)
  print(grad01.shape)

  # gradient for the output layer weights
  grad12 = self.hidden_out.T @ (error_term * self._delsigmoid(self.output_final))
  print("grad12: ", grad12)
  print(grad12.shape)

  # updating the weights by the learning rate times their gradient
  self.weights_01 += self.lr * grad01
  self.weights_12 += self.lr * grad12

  # update the biases the same way
  self.b01 += np.sum(self.lr * ((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) * 
  self._delsigmoid(self.hidden_out), axis=0)
  self.b12 += np.sum(self.lr * error_term * self._delsigmoid(self.output_final), axis=0)
 
  def classify(self, datapoint):
    datapoint = np.transpose(datapoint)
    if self.forward(datapoint) >= 0.5:
      return 1
    return 0
```

```
class MLP:
  def __init__(self, train_data, target, lr=0.1, num_epochs=100, num_input=2, num_hidden=2, num_output=1):
    self.train_data = train_data
    self.target = target
    self.lr = lr
    self.num_epochs = num_epochs

    # initialize both sets of weights and biases randomly
    # - weights_01: weights between input and hidden layer
    # - weights_12: weights between hidden and output layer
    self.weights_01 = np.random.uniform(size=(num_input, num_hidden))
    self.weights_12 = np.random.uniform(size=(num_hidden, num_output))

    # - b01: biases for the hidden layer
    # - b12: bias for the output layer
    self.b01 = np.random.uniform(size=(1,num_hidden))
    self.b12 = np.random.uniform(size=(1,num_output))

    self.losses = []


  def update_weights(self):
    # Calculate the squared error
    loss = 0.5 * (self.target - self.output_final) ** 2
    print(loss)
    self.losses.append(np.sum(loss))
    error_term = (self.target - self.output_final)

    # the gradient for the hidden layer weights
    grad01 = self.train_data.T @ (((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) * self._delsigmoid(self.hidden_out))
    print("grad01: ", grad01)
    print(grad01.shape)

    # the gradient for the output layer weights
    grad12 = self.hidden_out.T @ (error_term * self._delsigmoid(self.output_final))
    print("grad12: ", grad12)
    print(grad12.shape)

    # updating the weights by the learning rate times their gradient
    self.weights_01 += self.lr * grad01
    self.weights_12 += self.lr * grad12

    # update the biases the same way
    self.b01 += np.sum(self.lr * ((error_term * self._delsigmoid(self.output_final)) * self.weights_12.T) * self._delsigmoid(self.hidden_out), axis=0)
    self.b12 += np.sum(self.lr * error_term * self._delsigmoid(self.output_final), axis=0)

  def _sigmoid(self, x):
    return 1 / (1 + np.exp(-x))

  def _delsigmoid(self, x):
    return x * (1 - x)

  def forward(self, batch):
    self.hidden_ = np.dot(batch, self.weights_01) + self.b01
    self.hidden_out = self._sigmoid(self.hidden_)
    self.output_ = np.dot(self.hidden_out, self.weights_12) + self.b12
    self.output_final = self._sigmoid(self.output_)
    return self.output_final

  def classify(self, datapoint):
    datapoint = np.transpose(datapoint)
    if self.forward(datapoint) >= 0.5:
      return 1
    return 0

  def train(self):
    for _ in range(self.num_epochs):
      self.forward(self.train_data)
      self.update_weights()

  def plot(self, h=0.01):
  # setting plot properties like size, theme and axis limits
    sns.set_style('darkgrid')
    plt.figure(figsize=(20, 20))
    plt.axis('scaled')
    plt.xlim(-0.1, 1.1)
    plt.ylim(-0.1, 1.1)
    colors = {
    0: "ro",
    1: "go"
    }

    # plotting the four datapoints
    for i in range(len(self.train_data)):
      plt.plot([self.train_data[i][0]],
      [self.train_data[i][1]],
      colors[self.target[i][0]],
      markersize=20)
    x_range = np.arange(-0.1, 1.1, h)
    y_range = np.arange(-0.1, 1.1, h)

    # creating a mesh to plot decision boundary
    xx, yy = np.meshgrid(x_range, y_range, indexing='ij')
    Z = np.array([[self.classify([x, y]) for x in x_range] for y in y_range])

    # using the contourf function to create the plot
    plt.contourf(xx, yy, Z, colors=['red', 'green', 'green', 'blue'], alpha=0.4)


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

train_data = np.array(
 [ [0, 0],
 [0, 1],
 [1, 0],
 [1, 1]])
target_xor = np.array(
 [[0],
 [1],
 [1],
 [0]])
target_nand = np.array(
 [[1],
 [1],
 [1],
 [0]])
target_or = np.array(
 [[0],
 [1],
 [1],
 [1]])
target_and = np.array(
 [[0],
 [0],
 [0],
 [1]])

mlp = MLP(train_data, target_xor, 0.2, 5000)
mlp.train()
mlp.plot()
```


