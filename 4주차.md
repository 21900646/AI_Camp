# clustering
클러스터 내의 기준과 데이터들의 거리합 최소
조건>
![image](https://user-images.githubusercontent.com/69943167/125376449-fac1cf00-e3c5-11eb-8c2f-6253f4f4ddc5.png)
![image](https://user-images.githubusercontent.com/69943167/125376465-031a0a00-e3c6-11eb-8921-390d197594e6.png)

이 두개의 조건을 만족시키는 중앙점을 계산해야함.
클러스터 간의 거리가 멀면 오판할 가능성이 줄어든다.

![image](https://user-images.githubusercontent.com/69943167/125376557-39578980-e3c6-11eb-8969-03a2cc41727d.png)

predict 함수를 이용하면 점 위치 확인 가능(1과 0 사이에서 갑자기 바뀌는 위치)

각 기준점으로부터 euclidian 거리 계산.

```
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math 
%matplotlib inline

# 데이터 만들기
iris = load_iris()
iris_data = iris.data
iris_data_pd = pd.DataFrame(iris_data, columns=iris.feature_names)
print(iris_data_pd)

# 점으로 찍어보기
petals = pd.DataFrame(iris_data_pd.values[:, 2:4], columns = ['petal length (cm)', 'petal width (cm)'])
plt.scatter(petals.values[:, 0], petals.values[:,1])
plt.show()

# 색으로 나타내기
from sklearn.cluster import KMeans

plt.figure(figsize=(7,5))
km = KMeans(n_clusters=2, random_state=20)
km.fit(iris_data_pd.iloc[:, 2:4])
y_pred = km.predict(iris_data_pd.iloc[:, 2:4])
plt.scatter(iris_data_pd.iloc[:,2], iris_data_pd.iloc[:,3], c=y_pred)
plt.title("Clustering")
plt.xlabel('petal length')
plt.ylabel('petal width')

plt.show()

# 점의 위치 먼저 확인(다른 점 하나)
print (km.predict(iris_data_pd.iloc[:, 2:4]))

# 다른 점 하나 위치
print(iris_data_pd.iloc[98, 2:4])

# 기준점으로부터 euclidian 거리 계산
def distance(x1, y1, x2, y2):
  dx = x2 -  x1
  dy = y2 - y1
  squared = dx**2 + dy**2
  result = math.sqrt(squared)
  return result

print('0 cluster distance: ', distance(iris_data_pd.iloc[98, 2], iris_data_pd.iloc[98, 3], km.cluster_centers_[0][0], km.cluster_centers_[0][1]))
print('1 cluster distance: ', distance(iris_data_pd.iloc[98, 2], iris_data_pd.iloc[98, 3], km.cluster_centers_[1][0], km.cluster_centers_[1][1]))

# 클러스터의 분류 갯수를 바꿔보면서 확인
n_cluster = [3,4,6,12]

for i in n_cluster:
  count = 1
  km = KMeans(n_clusters= i, random_state=20)
  km.fit(iris_data_pd.iloc[:, 2:4])
  y_pred = km.predict(iris_data_pd.iloc[:, 2:4])
  plt.figure(count)
  plt.scatter(iris_data_pd.iloc[:,2], iris_data_pd.iloc[:, 3], c=y_pred)
  plt.title("Clustering = "+str(i))
  plt.xlabel('petal length')
  plt.ylabel('petal width')
  count=count+1
  plt.show()

# Voronoi 그래프로 묘사
km12 = KMeans(n_clusters = 8, random_state = 20)
km12.fit(iris_data_pd.iloc[:, 2:4])
y_pred12 = km12.predict(iris_data_pd.iloc[:, 2:4])
plt.title("Clustering")
plt.xlabel("petal length")
plt.ylabel('petal width')
graph1 = plt.scatter(iris_data_pd.iloc[:, 2], iris_data_pd.iloc[:, 3], c=y_pred12)
plt.show()

h = .02 #mash point
km12 = KMeans(n_clusters = 8, random_state=20) #n_clusters의 개수에 따라 범위가 달라진다)
km12.fit(iris_data_pd.iloc[:, 2:4])
y_pred12 = km12.predict(iris_data_pd.iloc[:, 2:4])

# 경계선에 컬러주기
x_min, x_max = iris_data_pd.iloc[:, 2].min() - 1, iris_data_pd.iloc[:, 2].max() + 1 
y_min, y_max = iris_data_pd.iloc[:, 3].min() - 1, iris_data_pd.iloc[:, 3].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))       

# 라벨 포인트 모으기
Z = km12.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot 
Z = Z.reshape(xx.shape) 
plt.figure(1) 
plt.clf() 
plt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap=plt.cm.Paired, aspect='auto', 
origin='lower') 
plt.plot(iris_data_pd.iloc[:, 2], iris_data_pd.iloc[:, 3], 'bo', markersize=2) 

# Plot the centroids as a white X 
centroids = km12.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker='^', s=16, linewidths=3, color='r', zorder=10)
plt.xlim(x_min, x_max) 
plt.ylim(y_min, y_max)
plt.xticks(()) 
plt.yticks(())
plt.show()

# 비선형 경계를 가지는 자료의 경우
from sklearn.datasets import make_moons
X, y = make_moons(200, noise=0.05, random_state = 0)

labels = KMeans(2, random_state = 0).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap = 'viridis')

# 데이터 준비
from sklearn.datasets import load_digits
digits = load_digits()
digits.data.shape

# 학습 및 예측
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 10, random_state = 0)
clusters = kmeans.fit_predict(digits.data)

kmeans.cluster_centers_.shape

# 클러스터 중심 확인
fig, ax = plt.subplots(2, 5, figsize=(8,3))
centers = kmeans.cluster_centers_.reshape(10,8,8)
for axi, center in zip(ax.flat, centers):
  axi.set(xticks=[], yticks=[])
  axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)
```

개수를 알고 있어야함.

## nearest neighbor
가장 가까운 두개를 묶어서 계층을 만든다.
scipy 패키지를 사용 -> 어떻게 트리가 그려졌는지
centroid, average, median, ward 총 4가지

## hierachical clustering
```
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
%matplotlib inline

iris = load_iris()
iris_data = iris.data
iris_data_pd = pd.DataFrame(iris_data, columns = iris.feature_names)
print(iris_data_pd)

from sklearn.cluster import AgglomerativeClustering

linkage = ["complete", "average", "ward"]
for idx, i in enumerate(linkage):
  plt.figure(idx)
  hier = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage=i)
  hier.fit(iris_data_pd.iloc[:, 2:4])
  plt.scatter(iris_data_pd.iloc[:, 2], iris_data_pd.iloc[:,3], c=hier.labels_)
  plt.title("Clustering : "+i)
  plt.xlabel('petal length')
  plt.ylabel('petal width')
plt.show()

# 어떻게 그려진 것인지 구도 찾기
from scipy.cluster import hierarchy

hierar = hierarchy.linkage(iris_data_pd.iloc[:, 2:4], 'complete')
plt.figure(figsize=(20,20))
dn = hierarchy.dendrogram(hierar)

hierar = hierarchy.linkage(iris_data_pd.iloc[:, 2:4], 'single')
plt.figure(figsize=(20,20))
dn = hierarchy.dendrogram(hierar)
```

## DBSCAN
core object로 등록해서 클러스터를 생성했으면 다음 원소로 넘어감.
eps (엡실론)을 수정. 
```
# 데이터 가져오기
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
%matplotlib inline

iris = load_iris()
iris_data = iris.data
iris_data_pd = pd.DataFrame(iris_data, columns = iris.feature_names)
print(iris_data_pd)

# dot 그래프로 나타내기
from sklearn.cluster import DBSCAN

db = DBSCAN(eps = 0.5, min_samples= 10)
db.fit(iris_data_pd.iloc[:,2:4])
y_pred = db.fit_predict(iris_data_pd.iloc[:, 2:4])
plt.scatter(iris_data_pd.iloc[:,2], iris_data_pd.iloc[:,3], c=y_pred)
plt.title("Clustering")
plt.xlabel('petal length')
plt.ylabel('petal width')

plt.show()

# label 나타내기
db.labels_

# 두 가지 색
db.set_params(eps = 0.5, min_samples = 5)
y_pred = db.fit_predict(iris_data_pd.iloc[:, 2:4])
print(db.labels_)
plt.scatter(iris_data_pd.iloc[:,2], iris_data_pd.iloc[:,3], c=y_pred)
plt.title("Clustering")
plt.xlabel('petal length')
plt.ylabel('petal width')
plt.show()

# 한 가지 색
eps = [0.1, 0.5, 1.0, 1.5]
for idx, i in enumerate(eps):
  plt.figure(idx)
  db.set_params(eps = i, min_samples = 10)
  y_pred = db.fit_predict(iris_data_pd.iloc[:, 2:4])
  print(db.labels_)
  plt.scatter(iris_data_pd.iloc[:,2], iris_data_pd.iloc[:,3], c=y_pred)
  plt.title("Density Clustering")
  plt.xlabel('petal length')
  plt.ylabel('petal width')
  plt.show()
  
# moon 만들기
from sklearn.datasets import make_moons
X, y = make_moons(200, noise=0.05, random_state = 0)

from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
D_labels = DBSCAN(eps=0.3, min_samples = 15).fit_predict(X)
plt.scatter(X[:,0], X[:,1], c=D_labels, s=50, cmap='viridis')
 ```
 보라색 즉, 값이 -1인 애들은 core object가 될 수 없으며, cluster도 안된다.
 
 ## PCA
 차원을 축소함. 어느 쪽이 영향을 많이 미치고, 어느쪽이 안미치는 지 찾는 것.
 ```
 import pandas as pd
import numpy as np
rng = np.random.RandomState(1)
X = np.dot(rng.rand(2,2), rng.randn(2, 200)).T
plt.scatter(X[:,0], X[:,1])
plt.axis('equal')

from sklearn.decomposition import PCA
mypca = PCA(n_components = 2)
mypca.fit(X)

print(mypca.components_)
print(mypca.explained_variance_)

def draw_vector(v0, v1, ax=None):
  ax = ax or plt.gca()
  arrowprops = dict(color='red', arrowstyle='simple', linewidth=2, shrinkA=0, shrinkB=0)
  ax.annotate('', v1, v0, arrowprops=arrowprops)

plt.scatter(X[:,0], X[:,1], alpha=0.2)
for length, vector in zip(mypca.explained_variance_, mypca.components_):
  v= vector * 3 * np.sqrt(length)
  draw_vector(mypca.mean_, mypca.mean_ + v)
plt.axis('equal')

dimpca = PCA(n_components=1)
dimpca.fit(X)
X_pca = dimpca.transform(X)
print('original shape: ', X.shape)
print('transformed shape: ', X_pca.shape)

X_new = dimpca.inverse_transform(X_pca)
plt.scatter(X[:,0], X[:,1], alpha=0.2)
plt.scatter(X_new[:,0], X_new[:,1], alpha=0.8)
plt.axis('equal')
```
```
# 얼굴 가져오기
from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=60)
print(faces.target_names)
print(faces.images.shape)

#얼굴 특징 추출
from sklearn.decomposition import PCA
face_pca = PCA(150)
face_pca.fit(faces.data)

fig, axes=plt.subplots(3,8,figsize=(9,4), subplot_kw={'xticks':[], 'yticks':[]}, gridspec_kw=dict(hspace=0.1, wspace=0.1))
for i, ax in enumerate(axes.flat):
  ax.imshow(face_pca.components_[i].reshape(62, 47), cmap='bone')
```

## 뉴런
![image](https://user-images.githubusercontent.com/69943167/125573275-cb4c0618-19a7-4a24-93aa-3f134bc1eda4.png)
not 연산자 -> -x - 0.5

